# K-Nearest Neighbor

E’ un lazy learner, ovvero esegue i necessari calcoli solo nella fase di test quando è
richiesto di produrre una predizione
● Utilizza il concetto di vicinanza tra i punti per fare predizioni; basa la classificazione
su un apprendimento per analogia, comparando le tuple del test set con le tuple del
training set che ci assomigliano; di default si usa la distanza euclidea per stabilire
quanto le tuple si assomigliano
● k sono il numero di vicini ispezionati; un k troppo piccolo, come ad esmepio k=1, non
va bene perché il modello così sarebbe poco affidabile in quanto baserebbe la sua
scelta su una sola istanza o comunque poche istanze del training set e questo
renderebbe il modello troppo sensibile alle istanze rumorose del dataset
● Con k = +infinito la predizione del modello sarà la moda del dataset e quindi troppo
generale e superficiale; si consiglia quindi di fare tuning del parametro
● Quando trovo le k istanze del trianing set più simili all’istanza x che voglio
classificare, dirò che l’etichetta di x è la moda delle etichette delle k istanze, ovvero la
più frequente. Nel caso di regressione l’etichetta di x coincide con la media delle
etichette delle k istanze più vicine/simili a x.
● La distanza euclidea ha un difetto: dà ad ogni feature lo stesso peso; per avere più
proporzione nelle nostre considerazioni possiamo armonizzare id ati facendoli
rientrare nello stesso range usando la librearia MinMaxScaler (tipicamente si mappa
tra 0 e 1 ma è variabile; è molto sensibile al rumore dei dati). Ne otteniamo un
miglioramento dell’accuracy
● Un altro modo di scalare i dati è usando uno StandardScaler. Lo StandardScaler non
rimappa ogni feature in un intervallo tra 0 e 1, bensì dipende dalla varianza, questo
vuol dire ceh una feature con grande varianza potrebbe avere un fote impatto sulla
varianza complessiva. Il MinMaxScaler è solitamente più sensibile agli outliers ma dà
valori più uniformi alle feature
● Quando crei l’oggetto del k-nn classifier è possibile passare al campo metric un
callable, ovvero una funzione che crei tu e che, in questo caso, calcola la distanza in
maniera customizzata
● Complessità computazionale: O(k*logN) con alberi di ricerca multi-dimensionali, più
altri costi per il clustering- based filtering